{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import delle librerie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os, glob, cv2\n",
    "import mediapipe as mp\n",
    "import pandas\n",
    "import LipLandmarks as lp\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esempio di Lip Feature Extraction\n",
    "\n",
    "Per estrapolare dati associati alle caratteristiche biometriche delle labbra dobbiamo riconscere se all'interno di una data immaginare è presente o meno un volto (Face Detection)\n",
    "\n",
    "Per questo faremo ricorso all'uso della libreria MediaPipe.\n",
    "\n",
    "Ecco una funzione che usa la libreria MediaPipe per restuire il risultato della face detection di un immagine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza la libreria Mediapipe per la face mesh\n",
    "def detect_face(image_name):\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "    # Carica l'immagine di esempio di Obama\n",
    "    sample_image = cv2.imread(\"obama.jpg\")\n",
    "\n",
    "    # Converte l'immagine in RGB\n",
    "    rgb_frame = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Processa l'immagine con Mediapipe per ottenere i punti della face mesh\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "    face_landmarks = results.multi_face_landmarks\n",
    "    \n",
    "    return results, sample_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando la face detection va a buon fine, cioè quando MediaPipe è in grado di riconoscere la presenza del volto in una data immagine, individua la posizione di 478 landmarks di rilevante importanza biometrica.\n",
    "\n",
    "Ecco un esempio di face detection su un'immagine usando la funzione precedente.\n",
    "\n",
    "Il risultato della funzione usato per mostare sull'immagine del volto i landmark individuati nella fase di face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_face_with_landmarks(results,sample_image):\n",
    "\n",
    "    face_landmarks = results.multi_face_landmarks\n",
    "\n",
    "    # Importa le librerie di disegno da Mediapipe\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if face_landmarks:\n",
    "        for face_landmark in face_landmarks:\n",
    "            # Disegna i landmarks sull'immagine\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=rgb_frame,\n",
    "                landmark_list=face_landmark,\n",
    "                connections=None, # senza connessioni\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=3, circle_radius=1),\n",
    "                connection_drawing_spec=None\n",
    "            )\n",
    "\n",
    "    # Visualizza l'immagine con i landmarks sovrapposti\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.show()\n",
    "\n",
    "image_name = \"obama.jpg\"\n",
    "results, sample_image = detect_face(image_name)\n",
    "print_face_with_landmarks(results, sample_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di questi 478 landmarks che MediaPipe individua, quando la face-detection va a buon fine, siamo interessati soltanto a quelli associati all'area labiale. \n",
    "\n",
    "Il file LipLandmarks.py contiene un frozen-set di 20 coppie di landmarks di nostro interesse, landmarks coinvolti nell'area labiale.\n",
    "\n",
    "L'insieme di coppie di landmarks definite nel file, considerando ogni coppia come un soggetto, delineano il contorno delle labbra.\n",
    "\n",
    "Usiamo il file LipLandmark.py per disegnare i 20 segmenti associati alle 20 coppie di landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"assets/obama.jpg\"\n",
    "\n",
    "results, sample_image = detect_face(image_name)\n",
    "\n",
    "height, width, _ = sample_image.shape\n",
    "\n",
    "face_landmarks = results.multi_face_landmarks\n",
    "\n",
    "# Importa le librerie di disegno da Mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "rgb_frame = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sample_image_landmarks = []\n",
    "sample_list_of_euclidean_distances = []\n",
    "\n",
    "iterator = iter(lp.lip_landmarks)\n",
    "for j in range(0, len(lp.lip_landmarks)):\n",
    "     i = next(iterator)\n",
    "     if results and results.multi_face_landmarks:\n",
    "        point = results.multi_face_landmarks[0].landmark[i[0]]\n",
    "        node1_x = int(point.x * width)\n",
    "        node1_y = int(point.y * height)\n",
    "        sample_image_landmarks.append((node1_x,node1_y))\n",
    "        cv2.circle(sample_image,(node1_x,node1_y),3,(0,0,255),1)\n",
    "\n",
    "        point = results.multi_face_landmarks[0].landmark[i[1]]\n",
    "        node2_x = int(point.x * width)\n",
    "        node2_y = int(point.y * height)\n",
    "        sample_image_landmarks.append((node2_x,node2_y))\n",
    "        cv2.circle(sample_image,(node2_x,node2_y),3,(0,0,255),1)\n",
    "\n",
    "        cv2.line(sample_image,(node1_x,node1_y),(node2_x,node2_y), (0,255,0), thickness=1)\n",
    "        cv2.putText(sample_image, str(j+1), ((node1_x+node2_x)//2, (node1_y+node2_y)//2),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calcolo della distanza euclidea tra i punti\n",
    "        d = math.sqrt((node2_x - node1_x) ** 2 + (node2_y - node1_y) ** 2)\n",
    "        sample_list_of_euclidean_distances.append(d)\n",
    "        \n",
    "\n",
    "plt.imshow(cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "for i, elem in enumerate(sample_list_of_euclidean_distances):\n",
    "    print(\"Line \",i+1,\" lenght: \",elem)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni coppia di landmark definisce quindi un segmento, come ad esempio quello del labbro inferiore o di quello superiore.\n",
    "\n",
    "Secondo le nostre assunzioni, il contenuto biometricamente informativo risiede nella lunghezza di tali segmenti.\n",
    "\n",
    "La lunghezza di un segmento è calcolata come la distanza euclidea tra i due landmarks in esso coinvolti.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codice per la creazione del dataset\n",
    "\n",
    "Il nostro dataset è composto da video di soggetti in primo piano. Il nostro obiettivo è estrarne le caratteristiche biometriche dell'area labiale.\n",
    "\n",
    "L'estrapolazione di tali caratteristiche è un'operazione che si applica ai singoli frame dei video.\n",
    "\n",
    "Qui decidiamo, per ogni video, quanti frame considerare. Da ogni frame, verranno prelevate le caratteristiche biometriche delle labbra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seguente funzione, fornendo come parametro il nome del file (video) sul quale operare, considera di tale video soltanto gli ultimi `num_frames` frame del video\n",
    "\n",
    "Per ognuno di essi tenta la face detection quindi la lip feature extraction, cioè il calcolo della lunghezza dei 20 segmenti labbiali di notevole importanza biometrica.\n",
    "\n",
    "Restituisce un vettore di vettore di 20 colonne (tanti quanto sono, appunto, le lip features in un frame cioè i segmenti) e tante righe quanti sono i frame dai quali è riuscito a prelevare le lip-features, il cui esito è strettamente correlato all'esito della face-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ottieni_lista_distanze_euclidee(filename):\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - num_frames - 1)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    landmarks = []\n",
    "\n",
    "    list_of_euclidean_distances = []\n",
    "\n",
    "    while ret:\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        height, width, _ = frame.shape\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        iterator = iter(lp.lip_landmarks)\n",
    "        for j in range(0, len(lp.lip_landmarks)):\n",
    "            i = next(iterator)\n",
    "            \n",
    "            if results and results.multi_face_landmarks:\n",
    "                # Primo elemento della tupla\n",
    "                point = results.multi_face_landmarks[0].landmark[i[0]]\n",
    "                node1_x = int(point.x * width)\n",
    "                node1_y = int(point.y * height)\n",
    "                landmarks.append((node1_x, node1_y))\n",
    "                cv2.circle(frame, (node1_x, node1_y), 2, (255, 255, 255), -1)\n",
    "\n",
    "                # Secondo elemento della tupla\n",
    "                point = results.multi_face_landmarks[0].landmark[i[1]]\n",
    "                node2_x = int(point.x * width)\n",
    "                node2_y = int(point.y * height)\n",
    "                landmarks.append((node2_x, node2_y))\n",
    "                cv2.circle(frame, (node2_x, node2_y), 2, (255, 255, 255), -1)\n",
    "\n",
    "                # Calcolo della distanza euclidea tra i punti\n",
    "                d = math.sqrt((node2_x - node1_x) ** 2 + (node2_y - node1_y) ** 2)\n",
    "                list_of_euclidean_distances.append(d)\n",
    "\n",
    "    return list_of_euclidean_distances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(csv_filename):\n",
    "    with open(csv_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        header_string = []\n",
    "        for i in range(20):\n",
    "            header_string.append(\"Feature \" + str(i))\n",
    "        header_string.append(\"Label\")\n",
    "        writer.writerow(header_string)\n",
    "\n",
    "        os.chdir(\"Dataset\")\n",
    "        video_dir = os.listdir()\n",
    "\n",
    "        for directory in video_dir:\n",
    "            if os.path.isdir(directory):\n",
    "                files = glob.glob(directory + \"/*.avi\")\n",
    "\n",
    "                for video in tqdm(files, desc=directory, ncols=100):\n",
    "                    res = ottieni_lista_distanze_euclidee(video)\n",
    "                    # Se necessario, cambiare qui il metodo di prelievo della label\n",
    "                    video_label = str(''.join(video.split(\"\\\\\")[1].split(\".\")[0].split(\"_\")[:4]))\n",
    "                    if np.shape(res)[0] == 20 * num_frames:\n",
    "                        res_split = np.array_split(res, num_frames)\n",
    "                        for i in range(num_frames):\n",
    "                            info_row = res_split[i]\n",
    "                            writer.writerow(np.append(info_row, video_label))\n",
    "        os.chdir(\"..\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(csv_filename):\n",
    "\n",
    "    df = pandas.read_csv(csv_filename)\n",
    "    df.sort_values(by=['Label'])\n",
    "\n",
    "    x = df.iloc[:, :-1].values\n",
    "    y = df['Label'].values\n",
    "\n",
    "    j = 0\n",
    "    last_label = y[0]\n",
    "    y[0] = j\n",
    "\n",
    "    for i in range(1, len(y)):\n",
    "        to_print = str(y[i])\n",
    "        if y[i] == last_label:\n",
    "            y[i] = j\n",
    "        else:\n",
    "            last_label = y[i]\n",
    "            j = j + 1\n",
    "            y[i] = j\n",
    "\n",
    "    x = scale(x)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    data = {'x_train': x_train,\n",
    "            'x_test': x_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo il dataset chiamando la funzione `create_csv(nome_csv_da_creare)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv(\"dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
